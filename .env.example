# Environment Configuration for DB Spark Application

# Deployment Mode - Toggle between local_small and aws_large
# local_small: Small local Spark cluster for development
# aws_large: Large AWS EC2 Spark cluster for production
DEPLOYMENT_MODE=local_small

# Flask Configuration
FLASK_ENV=development
FLASK_DEBUG=true
PORT=5000
HOST=0.0.0.0
LOG_LEVEL=INFO

# Spark Configuration (automatically configured based on DEPLOYMENT_MODE)
SPARK_MASTER_URL=spark://your-ec2-ip:7077
# Note: This is only used in aws_large mode
# local_small mode uses local[*] automatically

# AWS Configuration (required for aws_large mode)
AWS_REGION=us-west-2
S3_BUCKET=your-spark-data-bucket-name
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
# Note: For EC2 instances, use IAM roles instead of keys

# Database Configuration (if using traditional DB alongside Spark)
DATABASE_URL=postgresql://user:password@localhost:5432/dbname

# Security
SECRET_KEY=your-secret-key-here

# Monitoring and Logging
ENVIRONMENT=development
SENTRY_DSN=your-sentry-dsn-if-using

# Frontend URL (for CORS)
FRONTEND_URL=http://localhost:3000

# Vercel Configuration (for deployment)
VERCEL_URL=https://your-app.vercel.app

# ==============================================
# DEPLOYMENT MODE CONFIGURATIONS
# ==============================================

# LOCAL_SMALL MODE:
# - Uses local Spark with minimal resources
# - File-based data storage
# - No S3 required
# - Perfect for development and testing
# - Set DEPLOYMENT_MODE=local_small

# AWS_LARGE MODE:
# - Uses AWS EC2 Spark cluster
# - S3 for data storage
# - Requires S3_BUCKET and AWS credentials
# - Optimized for production workloads
# - Set DEPLOYMENT_MODE=aws_large